{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2750c39f",
   "metadata": {},
   "source": [
    "### Программа для получения информации по всем строящимся жк в РК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eee75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "814f5556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_page_small(city, amount): ### Функция парсит все ЖК из города, где меньше 13 строящихся ЖК. Передаётся явное количество ЖК, чтобы забрать именно столько\n",
    "                                          ###  Это нужно из-за отсутствия Пейджера в нижней части страницы. В остальных случаях мы ориентируемся по нему.\n",
    "    url = f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1'\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.scheme + '://' + parsed_url.netloc\n",
    "    response = requests.get(url) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        # join domain to path\n",
    "        full_url = urljoin(domain, link['href']) \n",
    "        links.append(full_url)\n",
    "    page_links = [] ### Здесь будем забирать после слова \"новостройки\" ровно amount жк\n",
    "    page_links = links[links.index('https://special.kolesa.group/novostroiki') + 1 : links.index('https://special.kolesa.group/novostroiki') + amount + 1]\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a56c245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_first_page(city): ### Мы будем парсить первую и вторую страницу города отдельно из-за их отличия в html коде от других страниц\n",
    "    url = f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1'\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.scheme + '://' + parsed_url.netloc\n",
    "    response = requests.get(url) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        # join domain to path\n",
    "        full_url = urljoin(domain, link['href']) \n",
    "        links.append(full_url)\n",
    "    page_links = []  ### Будем забирать от \"новостройки\" до пейджера.\n",
    "    page_links = links[links.index('https://special.kolesa.group/novostroiki') + 1 : links.index(f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1')]\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55a12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_second_page(city):\n",
    "    url = f\"https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1&page=2\"\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.scheme + '://' + parsed_url.netloc\n",
    "    response = requests.get(url) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        # join domain to path\n",
    "        full_url = urljoin(domain, link['href']) \n",
    "        links.append(full_url)\n",
    "    page_links = []\n",
    "    page_links = links[links.index('https://special.kolesa.group/novostroiki') + 1 : links.index(f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1')]\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca50fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_count(city): ### Функция забирает из Пейджера количество страниц с ЖК в данном городе\n",
    "    url = f\"https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    pager = soup.select('.complex-pager')\n",
    "    pure_pager = pager[0].text.replace('  ', '').replace('\\n', '') ### Чистим html мусор\n",
    "    if len(pure_pager) > 13: ### Если количество страниц больше 8, то из символов, к примеру, '1' и '4' мы делаем число 14\n",
    "        pages = int(pure_pager[pure_pager.index('Д') - 1]) + int(pure_pager[pure_pager.index('Д') - 2]) * 10\n",
    "    else: ### иначе, просто забираем последнюю цифру перед словом \"Дальше\" и переводим её в int\n",
    "        pages = int(pure_pager[pure_pager.index('Д') - 1])\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adeb296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_other_pages(city, list_of_pages): ### Функция парсит все остальные страницы по данному городу(начиная с 3)\n",
    "    all_pages = []\n",
    "    pages_amount = pages_count(city)\n",
    "    for page in range(3, pages_amount + 1): ### Цикл для прохода по всем остальным страницам новостроек\n",
    "        url = f\"https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1&page={page}\"\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.scheme + '://' + parsed_url.netloc\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "         #join domain to path\n",
    "            full_url = urljoin(domain, link['href']) \n",
    "            links.append(full_url)\n",
    "        page_links = []\n",
    "        page_links = links[links.index('https://special.kolesa.group/novostroiki') + 1 : links.index(f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1') - 1]\n",
    "        list_of_pages.append(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f83748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_buildings_amount(city): ### Функция считает количество ЖК в данном городе\n",
    "    url = f'https://krisha.kz/complex/search/{city}/?ceiling=&state[]=1'\n",
    "    response1 = requests.get(url)\n",
    "    soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "    res = soup.select(\".complex-heading__count\")\n",
    "    x = res[0].text.replace('\\n', '').replace('  ', '') ### Чистим мусор\n",
    "    return int(x[0 : x.index('н')]) ### Забираем из строки вида \"166новостроек\" именно \"166\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dafe2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_building_page(url): ### Основная функция, которая возвращает список со всей информацией по ЖК\n",
    "    building_info = []\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    name = get_building_name(soup)\n",
    "    building_info.append(name)\n",
    "    price = get_price_offer(soup)\n",
    "    building_info.append(price)\n",
    "    main_info = get_building_info(soup)\n",
    "    building_info.append(main_info)\n",
    "    parameters = get_building_parameters(soup)\n",
    "    building_info.append(parameters)\n",
    "    building_info.append(url)\n",
    "    return building_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40060136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_parameters(soup): ### Функция возвращает Словарь с параметрами о ЖК (Класс жилья, этажность и т.д.)\n",
    "    parameters = {}\n",
    "    info_title = soup.find_all('dt') ### Title(пример: \"Класс:\")\n",
    "    info_data = soup.find_all('dd')  ### Data(пример: \"Комфорт\")\n",
    "    iter = 0\n",
    "    for elem in info_title:\n",
    "        if (elem.text).find('Внутри жилого комплекса') >= 0: ### Не забираем лишней информации.\n",
    "            break\n",
    "        else:\n",
    "            parameters[elem.text] = info_data[iter].text\n",
    "            iter += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5e4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_name(soup): ### Функция забирает со страницы название недвижимости. Возвращаемое значение - str\n",
    "    name_dirty = soup.select(\"h1\")\n",
    "    return ((name_dirty[0].text).replace('  ', '')).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b594ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_float(string_price): ### Переводим цену в тип float. Если цена в виде \"1.4млн\", то переводим её в \"1400000\"\n",
    "    if string_price.find('млн') >= 0:\n",
    "        return float(string_price[0:string_price.index('м')]) * 1000000\n",
    "    else:\n",
    "        return float(string_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02f85956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price_offer(soup): ### Функция забирает со страницы стоимость недвижимости. возвращемое значение - str\n",
    "    price_dirty = soup.select(\".offer__price\")\n",
    "    rexp = r'(?<=от\\s).*(?=<\\w)'\n",
    "    price_clean_ungrouped = (re.search(rexp, str(price_dirty)))\n",
    "    if price_clean_ungrouped:\n",
    "        price_clean = price_clean_ungrouped.group()\n",
    "    else:\n",
    "        return 'Undefined' ### если цена не указана\n",
    "    return to_float(price_clean.replace('\\xa0', '')) ### Избавляемся от дополнительных html-символов и возвращаем float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b30b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building_info(soup): ### Функция забирает со страницы информацию о недвижимости. Возвращает list of strs\n",
    "    info_dirty = soup.select(\".complex__sidebar-info-text\")\n",
    "    info_clean = []\n",
    "    for i in range(4):\n",
    "        text = info_dirty[i].text\n",
    "        info_clean.append((text.replace('  ', '')).replace('\\n', ''))\n",
    "    return info_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bf6d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_city_links(city): ### Функция собирает ссылки на все жк города\n",
    "    list_of_pages = []\n",
    "    buildings_amount = check_buildings_amount(city)\n",
    "    if buildings_amount < 13: ### если у населенного пункта только 1 страница\n",
    "        first_page_links = parse_first_page_small(city, buildings_amount)\n",
    "        list_of_pages.append(first_page_links)\n",
    "        return list_of_pages\n",
    "        \n",
    "    \n",
    "### Достаём ссылки первой страницы отдельно вне цикла из-за отличия её от других страниц\n",
    "    first_page_links = parse_first_page(city)\n",
    "    \n",
    "### Заносим ссылки первой страницы в список, где будут храниться все ссылки на все новостройки\n",
    "    list_of_pages.append(first_page_links)\n",
    "    \n",
    "### Достаём ссылки на ЖК со второй страницы отдельно вне цикла из-за отличия её от других страниц\n",
    "    second_page_links = parse_second_page(city)\n",
    "    \n",
    "### Заносим ссылки второй страницы в список, где будут храниться все ссылки на все новостройки    \n",
    "    list_of_pages.append(second_page_links)\n",
    "    \n",
    "### Достаём ссылки на ЖК со всех остальных страниц и сразу заносим в общий список\n",
    "    other_pages_links = parse_other_pages(city, list_of_pages)\n",
    "    return list_of_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e42eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_city_information(city): ### Функция получает всю информацию по всем строящимся ЖК с данного города\n",
    "    links = parse_city_links(city)\n",
    "    dict_links = {}\n",
    "    dict_links = dict(enumerate(links, start = 1))\n",
    "    information_list = []\n",
    "    for key in dict_links.keys():\n",
    "        for elem in dict_links[key]:\n",
    "            information_list.append(parse_building_page(elem))\n",
    "    return information_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e91a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_xlsx(city, writer): ### Функция заносит информацию о строящихся ЖК в городе в excel файл в отдельный лист, который соотвествует городу.\n",
    "    information = parse_city_information(city)\n",
    "    d1 = {}\n",
    "    d1[\"name\"], d1[\"price\"], d1[\"info\"], d1[\"status\"], d1[\"year_of_building\"], d1[\"address\"], d1[\"developer\"], d1[\"class\"], d1[\"parameters\"], d1[\"link\"] = [], [], [], [], [], [], [], [], [], []\n",
    "    for elem in information:\n",
    "        d1[\"name\"].append(elem[0])\n",
    "        d1[\"price\"].append(elem[1])\n",
    "        d1[\"info\"].append(elem[2])\n",
    "        d1[\"status\"].append(elem[2][0])\n",
    "        d1[\"year_of_building\"].append(elem[2][1])\n",
    "        d1[\"address\"].append(elem[2][2])\n",
    "        d1[\"developer\"].append(elem[2][3])\n",
    "        d1[\"class\"].append(elem[3]['Класс жилья'])\n",
    "        #elem[3].pop('Класс жилья', None)\n",
    "        d1[\"parameters\"].append(elem[3])\n",
    "        d1[\"link\"].append(elem[4])\n",
    "    df = pd.DataFrame(d1)\n",
    "    df.to_excel(writer, sheet_name=f\"{city}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61b210ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cities_list(): ### Функция парсит все названия городов в формате сайта Крыша.кз (например: \"almaty\", \"astana\")\n",
    "    url = \"https://krisha.kz/complex/search/foreign/?ceiling=&state[]=1\"\n",
    "    response1 = requests.get(url)\n",
    "    soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "    res = soup.select(\".seo-links__city_list\") ### Забираем из списка городов в нижней части страницы\n",
    "    x = res[0].find_all('a') ### <a href=\"/complex/search/almaty/\"> Алматы </a> Из такой строки мы забираем именно \"almaty\"\n",
    "    cities_list = []\n",
    "    for elem in x:\n",
    "        cities_list.append((str(elem)[str(elem).find('search') + 7:str(elem).index('>') - 2]))\n",
    "    return cities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de89e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#from random import randint\n",
    "#from time import sleep\n",
    "\n",
    "def current_information(name_of_document): ### Функция пробегается по всем городам и заносит данные в указанный Excel файл по разным листам\n",
    "    cities_list = get_cities_list()\n",
    "    writer = pd.ExcelWriter(name_of_document, engine = \"xlsxwriter\")\n",
    "    for elem in cities_list:\n",
    "        #value_sleep = randint(1,5) ### Нужно в случае, если Крыша блокирует парсинг\n",
    "        #sleep(value_sleep)\n",
    "        print(elem)\n",
    "        to_xlsx(elem, writer)\n",
    "    writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e5f7e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "almaty\n",
      "astana\n",
      "shymkent\n",
      "karagandinskaja-oblast-abaj\n",
      "almatinskaja-oblast-abaj\n",
      "akmol\n",
      "aksaj\n",
      "aktau\n",
      "aktobe\n",
      "atyrau\n",
      "besagash-dzerzhinskoe\n",
      "boroldaj-burundaj\n",
      "borovoe\n",
      "zhanaozen\n",
      "irgeli\n",
      "karabulak\n",
      "karaganda\n",
      "kaskelen\n",
      "kokshetau\n",
      "konaev\n",
      "kostanaj\n",
      "kosshy\n",
      "kyzylorda\n",
      "otegen-batyr\n",
      "pavlodar\n",
      "petropavlovsk\n",
      "semej\n",
      "talgar\n",
      "taldykorgan\n",
      "taraz\n",
      "tuzdybastau-kalinino\n",
      "turkestan\n",
      "uralsk\n",
      "ust-kamenogorsk\n",
      "shhuchinsk\n",
      "ekibastuz\n"
     ]
    }
   ],
   "source": [
    "current_information('All_regions.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
